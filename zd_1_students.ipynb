{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrolovYV/Business-analytics/blob/main/zd_1_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACpYdqb_Q4X5"
      },
      "source": [
        "# Задание_1: Python Business Analytics\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0xICACxQyo1"
      },
      "source": [
        "\n",
        "# Сегментация клиентов\n",
        "\n",
        "Мы будем использовать набор данных электронной коммерции о покупках пользователей и попробуем разработать модель, которая позволит нам сделать две вещи:\n",
        "\n",
        "1. Классификация клиентов по сегментам.\n",
        "2. Предвидеть покупки, которые совершит новый клиент в течение следующего года и\n",
        "    это, начиная с первой покупки, назначив им соответствующий кластер/сегмент \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1hcMkbFQypB"
      },
      "source": [
        "## ** Import dependancies **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "705714b1-870b-4f34-b5bf-cd027dafaefe",
        "_kg_hide-input": true,
        "_uuid": "bb40a7b23734d82876d812fab6daecd83a46368c",
        "id": "fpkAdetuQypE"
      },
      "source": [
        "from __future__ import division\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime, nltk, warnings\n",
        "import matplotlib.cm as cm\n",
        "import itertools\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn import preprocessing, model_selection, metrics, feature_selection\n",
        "from sklearn.model_selection import GridSearchCV, learning_curve\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import neighbors, linear_model, svm, tree, ensemble\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from IPython.display import display, HTML\n",
        "#import plotly.plotly as py\n",
        "#import plotly.graph_objs as go\n",
        "#from plotly.offline import init_notebook_mode,iplot\n",
        "#init_notebook_mode(connected=True)\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "#plt.rcParams[\"patch.force_edgecolor\"] = True\n",
        "#plt.style.use('fivethirtyeight')\n",
        "#mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUuBYcELQypZ"
      },
      "source": [
        "##   Data Preparation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1063f9e0-e494-4873-939f-8aa5ca40cc89",
        "_kg_hide-input": true,
        "_uuid": "227fc0cb1d5216d52e057e1d2d7debd0e29abe46",
        "scrolled": false,
        "id": "WF5UrStrQypb"
      },
      "source": [
        "# read the datafile\n",
        "df_initial = pd.read_csv('https://raw.githubusercontent.com/firmai/python-business-analytics/master/data/customer/data.csv',encoding=\"ISO-8859-1\",\n",
        "                         dtype={'CustomerID': str,'InvoiceID': str})\n",
        "print('Dataframe dimensions:', df_initial.shape)\n",
        "df_initial['InvoiceDate'] = pd.to_datetime(df_initial['InvoiceDate'])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAokjxnnQypw"
      },
      "source": [
        "# show first lines\n",
        "display(df_initial[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKIvmTbFQyqE"
      },
      "source": [
        "## Exploratory Data Analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3oFn3m3QyqH"
      },
      "source": [
        "### Identify null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uypjRoTjQyqK"
      },
      "source": [
        "# gives some infomation on columns types and number of null values\n",
        "tab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\n",
        "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
        "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()/df_initial.shape[0]*100).T.\n",
        "                         rename(index={0:'null values (%)'}))\n",
        "print ('-' * 10 + \" Display information about column types and number of null values \" + '-' * 10 )\n",
        "print \n",
        "display(tab_info)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQtaVruvQyqd"
      },
      "source": [
        "### Remove data entries \n",
        "Note: \n",
        " * If you are looking to the CustomerID column then there are $\\sim$25% data entries are   \n",
        "   null.\n",
        " * That means there are $\\sim$25% of data entries which aren't assigned to the any \n",
        "   customer(s).\n",
        " * It is impossible for us to map values for the customer and these entries. These is \n",
        "   usless for the current exercise.\n",
        " * Because of all the above points we are deleting these data entries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f9de6b67-a588-43ab-8f51-b28efdee9e32",
        "_kg_hide-input": true,
        "_uuid": "9b915fa18b311e8f93ac862bd49d08d90e03ca48",
        "id": "AXMssSZjQyqp"
      },
      "source": [
        "df_initial.dropna(axis = 0, subset = ['CustomerID'], inplace = True)\n",
        "print('Dataframe dimensions:', df_initial.shape)\n",
        "# gives some information on columns types and number of null values\n",
        "tab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\n",
        "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
        "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()/df_initial.shape[0]*100).T.\n",
        "                         rename(index={0:'null values (%)'}))\n",
        "display(tab_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "baf1ff2e-646b-468b-b7b4-68343f388387",
        "_uuid": "6b988d1dee3deecafd54f2b3555d1f84b509d213",
        "id": "J8y8ewdLQyq4"
      },
      "source": [
        "print('Duplicate data entries: {}'.format(df_initial.duplicated().sum()))\n",
        "df_initial.drop_duplicates(inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsSoMyKRQyrL"
      },
      "source": [
        "### Exploring data attributes\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TooI44Q-QyrN"
      },
      "source": [
        "#### ** Exploring the data attribute : Country **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "44abc17d-8858-457a-94e1-f012143fda87",
        "_kg_hide-input": true,
        "_uuid": "c4141f12a8b2c733539a75d398cadf0817ca0969",
        "id": "ydQR0lwPQyrS"
      },
      "source": [
        "temp = df_initial[['CustomerID', 'InvoiceNo', 'Country']].groupby(\n",
        "    ['CustomerID', 'InvoiceNo', 'Country']).count()\n",
        "temp = temp.reset_index(drop = False)\n",
        "countries = temp['Country'].value_counts()\n",
        "print('No. of cuntries in dataframe: {}'.format(len(countries)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxGUlXSaQyrl"
      },
      "source": [
        "temp_no_of_order_per_count = df_initial[['CustomerID','Country']].groupby(['Country']).count()\n",
        "temp_no_of_order_per_count = temp_no_of_order_per_count.reset_index(drop = False)\n",
        "\n",
        "print('-' * 10 + \" Contry-wise order calculation \"+ '-' * 10)\n",
        "print\n",
        "print (temp_no_of_order_per_count.sort_values(\n",
        "    by='CustomerID', ascending=False).rename(index=str,\n",
        "                                        columns={\"CustomerID\": \"Country wise number of order\"}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvj6x1WpQyr2"
      },
      "source": [
        "#### ** Exploring the data attribute : Customers and products **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl0C-Gn8Qyr6"
      },
      "source": [
        "The dataframe contains $\\sim$200,000 entries. What are the number of users and products in these entries ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "677f103d-d199-480c-bca4-fc08f7aa2e92",
        "_kg_hide-input": true,
        "_uuid": "dc2f4b48b76615721e6718efbd31fcd3faf16bec",
        "id": "1FJoZIYJQyr8"
      },
      "source": [
        "pd.DataFrame([{'products': len(df_initial['StockCode'].value_counts()),    \n",
        "               'transactions': len(df_initial['InvoiceNo'].value_counts()),\n",
        "               'customers': len(df_initial['CustomerID'].value_counts()),  \n",
        "              }], columns = ['products', 'transactions', 'customers'], \n",
        "              index = ['quantity'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OD5Cgh_QysN"
      },
      "source": [
        "As you can see that this dataset contain the recods of 4372 users who bought 3684 different items.\n",
        "There are $\\sim$22,000 transactions which are carried out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vldfi9MUQysQ"
      },
      "source": [
        "Now we need to explore the number of products purchased in every transaction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "72d6dede-4280-4afd-b61b-085ea8c73d67",
        "_kg_hide-input": true,
        "_uuid": "dd0d84bd4275a04e361b5b41924d11b7f6f2e9ff",
        "id": "Mfdktlp5QysS"
      },
      "source": [
        "temp = df_initial.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate'].count()\n",
        "nb_products_per_basket = temp.rename(columns = {'InvoiceDate':'Number of products'})\n",
        "nb_products_per_basket[:10].sort_values('CustomerID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sxncqc8Qysm"
      },
      "source": [
        "Points to be noted here: \n",
        "\n",
        "* There are some users who bought only comes one time on the E-commerce platform and purchased one   \n",
        "  item. The example of this kind of user is customerID 12346.  \n",
        "\n",
        "* There are some users who frequently buy large number of items per order. The example of this kind of \n",
        "  user is customerID 12347.  \n",
        "\n",
        "* If you notice Invoiceno data attribute then you can find out that there is prefix 'C' for one \n",
        "  invoice. This 'C' indicates that the particular transaction has been cancelled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOoI7nq3Qysw"
      },
      "source": [
        "#### ** Analysis of cancelled orders **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFVAPqwkQys1"
      },
      "source": [
        "We need to count the number of transactions corresponding to cancelled orders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "9b0e32d8-fc9c-4301-ac18-9c1d7cc5b54f",
        "_kg_hide-input": true,
        "_uuid": "076fba25ed8a2b38fddd83ff862fa21e7f790a11",
        "id": "rksey9dpQys3"
      },
      "source": [
        "nb_products_per_basket['order_cancelled'] = nb_products_per_basket['InvoiceNo'].apply(\n",
        "    lambda x:int('C' in x))\n",
        "display(nb_products_per_basket[:5])\n",
        "\n",
        "\n",
        "n1 = nb_products_per_basket['order_cancelled'].sum()\n",
        "n2 = nb_products_per_basket.shape[0]\n",
        "percentage = (n1/n2)*100\n",
        "print('Number of orders cancelled: {}/{} ({:.2f}%) '.format(n1, n2, percentage))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_JRIgaaQytD"
      },
      "source": [
        "Note that the number of cancelled transactions are quite large ( $\\sim$15% of the total number of transactions). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eJcLrXbQytH"
      },
      "source": [
        "Now, let's look at the first few lines of the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "2f985d42-e0b9-4281-8f30-43c2c13955b9",
        "_kg_hide-input": true,
        "_uuid": "54f5b8a4bc832c1c396419223f43c41b9b0b27de",
        "scrolled": true,
        "id": "kyMl3eg5QytS"
      },
      "source": [
        "display(df_initial.sort_values('CustomerID')[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ-gbcXOQyti"
      },
      "source": [
        "From the above output, we see that when an order is canceled, we have another transactions in the dataframe, mostly identical except for the **Quantity** and **InvoiceDate** variables. I decide to check if this is true for all the entries.\n",
        "To do this, I decide to locate the entries that indicate a negative quantity and check if there is *systematically* an order indicating the same quantity (but positive), with the same description (**CustomerID**, **Description** and **UnitPrice**):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "06e26580-014e-432d-ab59-b5ceebb816cb",
        "_kg_hide-input": false,
        "_uuid": "b16ddfdd36696a4a92ba15acd387e7eca0757f31",
        "scrolled": true,
        "id": "6VNO9gArQytk"
      },
      "source": [
        "df_check = df_initial[df_initial['Quantity'] < 0][['CustomerID','Quantity',\n",
        "                                                   'StockCode','Description','UnitPrice']]\n",
        "for index, col in  df_check.iterrows():\n",
        "    if df_initial[(df_initial['CustomerID'] == col[0]) & (df_initial['Quantity'] == -col[1]) \n",
        "                & (df_initial['Description'] == col[2])].shape[0] == 0: \n",
        "        print(df_check.loc[index])\n",
        "        print(15*'-'+'>'+' HYPOTHESIS NOT FULFILLED')\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttfi2mSlQytz"
      },
      "source": [
        "We see that the initial hypothesis is not fulfilled because of the existence of a  '_Discount_' entry. I check again the hypothesis but this time discarding the '_Discount_' entries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "50c6589e-4387-4a3e-a1ea-18bee5bb1dba",
        "_kg_hide-input": true,
        "_uuid": "3b375d17a84505d71b20dbbc43124aa6597a2ef2",
        "id": "y0WzdlbJQyt1"
      },
      "source": [
        "df_check = df_initial[(df_initial['Quantity'] < 0) & (df_initial['Description'] != 'Discount')][\n",
        "                                 ['CustomerID','Quantity','StockCode',\n",
        "                                  'Description','UnitPrice']]\n",
        "\n",
        "for index, col in  df_check.iterrows():\n",
        "    if df_initial[(df_initial['CustomerID'] == col[0]) & (df_initial['Quantity'] == -col[1]) \n",
        "                & (df_initial['Description'] == col[2])].shape[0] == 0: \n",
        "        print(index, df_check.loc[index])\n",
        "        print(15*'-'+'>'+' HYPOTHESIS NOT FULFILLED')\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdvqNSZDQyuQ"
      },
      "source": [
        "Once more, we find that the initial hypothesis is not verified. Hence, cancellations do not necessarily correspond to orders that would have been made beforehand.\n",
        "\n",
        "At this point, I decide to create a new variable in the dataframe that indicate if part of the command has been canceled. For the cancellations without counterparts, a few of them are probably due to the fact that the buy orders were  performed before December 2010 (the point of entry of the database). Below, I make a census of the cancel orders and check for the existence of counterparts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "af540729-739b-45b3-858f-f1facf9f8ae6",
        "_kg_hide-input": true,
        "_uuid": "6f5c10794e09eb3d0dc83889d74b2029d6d24756",
        "id": "IDzH6pjyQyuS"
      },
      "source": [
        "df_cleaned = df_initial.copy(deep = True)\n",
        "df_cleaned['QuantityCanceled'] = 0\n",
        "\n",
        "entry_to_remove = [] ; doubtfull_entry = []\n",
        "\n",
        "for index, col in  df_initial.iterrows():\n",
        "    if (col['Quantity'] > 0) or col['Description'] == 'Discount': continue        \n",
        "    df_test = df_initial[(df_initial['CustomerID'] == col['CustomerID']) &\n",
        "                         (df_initial['StockCode']  == col['StockCode']) & \n",
        "                         (df_initial['InvoiceDate'] < col['InvoiceDate']) & \n",
        "                         (df_initial['Quantity']   > 0)].copy()\n",
        "\n",
        "    # Cancelation WITHOUT counterpart\n",
        "    if (df_test.shape[0] == 0): \n",
        "        doubtfull_entry.append(index)\n",
        "   \n",
        "    # Cancelation WITH a counterpart\n",
        "    elif (df_test.shape[0] == 1): \n",
        "        index_order = df_test.index[0]\n",
        "        df_cleaned.loc[index_order, 'QuantityCanceled'] = -col['Quantity']\n",
        "        entry_to_remove.append(index)        \n",
        "   \n",
        "    # Various counterparts exist in orders: we delete the last one\n",
        "    elif (df_test.shape[0] > 1): \n",
        "        df_test.sort_index(axis=0 ,ascending=False, inplace = True)        \n",
        "        for ind, val in df_test.iterrows():\n",
        "            if val['Quantity'] < -col['Quantity']: continue\n",
        "            df_cleaned.loc[ind, 'QuantityCanceled'] = -col['Quantity']\n",
        "            entry_to_remove.append(index) \n",
        "            break            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoK91OmWQyuX"
      },
      "source": [
        "In the above function, I checked the two cases:\n",
        "1. a cancel order exists without counterpart\n",
        "2. there's at least one counterpart with the exact same quantity\n",
        "\n",
        "The index of the corresponding cancel order are respectively kept in the `doubtfull_entry` and `entry_to_remove` lists whose sizes are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f1d3a68d-fa59-4671-8be1-cdbb646ce13f",
        "_kg_hide-input": true,
        "_uuid": "d98a0917de35db7afe31c69c324cd32e934edd52",
        "scrolled": true,
        "id": "srle8SrPQyuZ"
      },
      "source": [
        "print(\"entry_to_remove: {}\".format(len(entry_to_remove)))\n",
        "print(\"doubtfull_entry: {}\".format(len(doubtfull_entry)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tsOmj-pQyvM"
      },
      "source": [
        "Among these entries, the lines listed in the doubtfull_entry list correspond to the entries indicating a cancellation but for which there is no command beforehand. In practice, I decide to delete all of these entries, which count respectively for $\\sim$ 1.4% and 0.2% of the dataframe entries.\n",
        "\n",
        "Now I check the number of entries that correspond to cancellations and that have not been deleted with the previous filter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "50f6c074-08cc-4c55-8285-d674d0d84b45",
        "_kg_hide-input": true,
        "_uuid": "c523275748742927f689725305e90ae6fdeb4136",
        "id": "FFtcZ4BSQyvR"
      },
      "source": [
        "df_cleaned.drop(entry_to_remove, axis = 0, inplace = True)\n",
        "df_cleaned.drop(doubtfull_entry, axis = 0, inplace = True)\n",
        "remaining_entries = df_cleaned[(df_cleaned['Quantity'] < 0) & (df_cleaned['StockCode'] != 'D')]\n",
        "print(\"nb of entries to delete: {}\".format(remaining_entries.shape[0]))\n",
        "remaining_entries[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMETCcqJQyva"
      },
      "source": [
        "If one looks, for example, at the purchases of the consumer of one of the above entries and corresponding to the same product as that of the cancellation, one observes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "86494812-a35a-49a8-a80d-5b2ef3b0913f",
        "_uuid": "5ded98b83a85e0fd038b1a5c5edb67d7773d41ee",
        "scrolled": true,
        "id": "s2djnyITQyve"
      },
      "source": [
        "df_cleaned[(df_cleaned['CustomerID'] == 14048) & (df_cleaned['StockCode'] == '22464')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UwK9mwxQyvr"
      },
      "source": [
        "We see that the quantity canceled is greater than the sum of the previous purchases.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Analysis of the StockCode**\n",
        "\n",
        "Above, it has been seen that some values of the ** StockCode ** variable indicate a particular transaction (i.e. D for _Discount_). I check the contents of this variable by looking for the set of codes that would contain only letters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "e00212c8-5c1e-4dda-a392-cbc68c1964b1",
        "_kg_hide-input": true,
        "_uuid": "57e546917a0ea9a59a0e1dc3e0f9179c7efa66b5",
        "scrolled": true,
        "id": "5O1VG4VRQyv2"
      },
      "source": [
        "list_special_codes = df_cleaned[df_cleaned['StockCode'].str.contains('^[a-zA-Z]+', regex=True)]['StockCode'].unique()\n",
        "list_special_codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "0a225335-7d6e-4c5b-a874-801e3f329f10",
        "_kg_hide-input": true,
        "_uuid": "ce078be30fea360c161b449a8cb666d98808e936",
        "id": "8yfe5SIMQyv-"
      },
      "source": [
        "for code in list_special_codes:\n",
        "    print(\"{:<15} -> {:<30}\".format(code, df_cleaned[df_cleaned['StockCode'] == code]['Description'].unique()[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWWeUivtQywg"
      },
      "source": [
        "We see that there are several types of peculiar transactions, connected e.g. to port charges or bank charges.\n",
        "\n",
        "\n",
        "___\n",
        "#### Analysis of Basket Price\n",
        "\n",
        "\n",
        "I create a new variable that indicates the total price of every purchase:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "3cbf20c0-0a44-49dc-96c3-ffd5455ddf0b",
        "_kg_hide-input": true,
        "_uuid": "5f070241e41d989ed3de0769d9f35f330d086415",
        "id": "AgQh64YIQywq"
      },
      "source": [
        "df_cleaned['TotalPrice'] = df_cleaned['UnitPrice'] * (df_cleaned['Quantity'] - df_cleaned['QuantityCanceled'])\n",
        "df_cleaned.sort_values('CustomerID')[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw5DS5B7Qywz"
      },
      "source": [
        "Each entry of the dataframe indicates prizes for a single kind of product. Hence, orders are split on several lines. I collect all the purchases made during a single order to recover the total order prize:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "5e4530b2-addf-4dc7-9ca8-065c26b73023",
        "_kg_hide-input": true,
        "_uuid": "653fd7be2e985cf4578af4306f40948926fb60b3",
        "id": "Dw9neckqQyw1"
      },
      "source": [
        "\n",
        "# sum of purchases / user & order\n",
        "temp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['TotalPrice'].sum()\n",
        "basket_price = temp.rename(columns = {'TotalPrice':'Basket Price'})\n",
        "\n",
        "# date of the order\n",
        "df_cleaned['InvoiceDate_int'] = df_cleaned['InvoiceDate'].astype('int64')\n",
        "temp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate_int'].mean()\n",
        "df_cleaned.drop('InvoiceDate_int', axis = 1, inplace = True)\n",
        "basket_price.loc[:, 'InvoiceDate'] = pd.to_datetime(temp['InvoiceDate_int'])\n",
        "\n",
        "# selection of significant entries\n",
        "basket_price = basket_price[basket_price['Basket Price'] > 0]\n",
        "basket_price.sort_values('CustomerID')[:6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgQWH8s8Qyw8"
      },
      "source": [
        "In order to have a global view of the type of order performed in this dataset, I determine how the purchases are divided according to total prizes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "25f72313-bc56-4a10-99b1-243f147b1756",
        "_kg_hide-input": true,
        "_uuid": "b1b30be7aa80d7a5287e6fd783b5d8cdbff4032d",
        "id": "MRn5y6fLQyw-"
      },
      "source": [
        "# Purchase count\n",
        "price_range = [0, 50, 100, 200, 500, 1000, 5000, 50000]\n",
        "count_price = []\n",
        "for i, price in enumerate(price_range):\n",
        "    if i == 0: continue\n",
        "    val = basket_price[(basket_price['Basket Price'] < price) &\n",
        "                       (basket_price['Basket Price'] > price_range[i-1])]['Basket Price'].count()\n",
        "    count_price.append(val)\n",
        "\n",
        "# Representation of the number of purchases / amount       \n",
        "plt.rc('font', weight='bold')\n",
        "f, ax = plt.subplots(figsize=(11, 6))\n",
        "colors = ['yellowgreen', 'gold', 'wheat', 'c', 'violet', 'royalblue','firebrick']\n",
        "labels = [ '{}<.<{}'.format(price_range[i-1], s) for i,s in enumerate(price_range) if i != 0]\n",
        "sizes  = count_price\n",
        "explode = [0.0 if sizes[i] < 100 else 0.0 for i in range(len(sizes))]\n",
        "ax.pie(sizes, explode = explode, labels=labels, colors = colors,\n",
        "       autopct = lambda x:'{:1.0f}%'.format(x) if x > 1 else '',\n",
        "       shadow = False, startangle=0)\n",
        "ax.axis('equal')\n",
        "f.text(0.5, 1.01, \"Distribution of order amounts\", ha='center', fontsize = 18);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QofNlcCPQyxL"
      },
      "source": [
        "It can be seen that the vast majority of orders concern relatively large purchases given that $\\sim$65% of purchases give prizes in excess of £ 200."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lf_i5ruQyxN"
      },
      "source": [
        "###  Analysis of the product categories\n",
        "\n",
        "In the data-frame, products are uniquely identified through the **StockCode** variable. A short description of the products is given in the **Description** variable. In this section, I intend to use the content of this latter variable in order to group the products into different categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-0RDZkeQyxO"
      },
      "source": [
        "#### Products Description\n",
        "\n",
        "\n",
        "As a first step, I extract from the **Description** variable the information that will prove useful. To do this, I use the following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "62aada7f-1d61-493e-a044-08fcc7bdfb81",
        "_kg_hide-input": true,
        "_uuid": "4ae364672f6cede623fd0e032e34d967e4f32ee1",
        "id": "7CSKWr8YQyxR"
      },
      "source": [
        "is_noun = lambda pos: pos[:2] == 'NN'\n",
        "\n",
        "def keywords_inventory(dataframe, colonne = 'Description'):\n",
        "    stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
        "    keywords_roots  = dict()  # collect the words / root\n",
        "    keywords_select = dict()  # association: root <-> keyword\n",
        "    category_keys   = []\n",
        "    count_keywords  = dict()\n",
        "    icount = 0\n",
        "    for s in dataframe[colonne]:\n",
        "        if pd.isnull(s): continue\n",
        "        lines = s.lower()\n",
        "        tokenized = nltk.word_tokenize(lines)\n",
        "        nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
        "        \n",
        "        for t in nouns:\n",
        "            t = t.lower() ; racine = stemmer.stem(t)\n",
        "            if racine in keywords_roots:                \n",
        "                keywords_roots[racine].add(t)\n",
        "                count_keywords[racine] += 1                \n",
        "            else:\n",
        "                keywords_roots[racine] = {t}\n",
        "                count_keywords[racine] = 1\n",
        "    \n",
        "    for s in keywords_roots.keys():\n",
        "        if len(keywords_roots[s]) > 1:  \n",
        "            min_length = 1000\n",
        "            for k in keywords_roots[s]:\n",
        "                if len(k) < min_length:\n",
        "                    clef = k ; min_length = len(k)            \n",
        "            category_keys.append(clef)\n",
        "            keywords_select[s] = clef\n",
        "        else:\n",
        "            category_keys.append(list(keywords_roots[s])[0])\n",
        "            keywords_select[s] = list(keywords_roots[s])[0]\n",
        "                   \n",
        "    print(\"number of keywords in variable '{}': {}\".format(colonne,len(category_keys)))\n",
        "    return category_keys, keywords_roots, keywords_select, count_keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caJdP6kxQyxX"
      },
      "source": [
        "This function takes as input the dataframe and analyzes the content of the **Description** column by performing the following operations:\n",
        "\n",
        "- extract the names (proper, common) appearing in the products description\n",
        "- for each name, I extract the root of the word and aggregate the set of names associated with this particular root\n",
        "- count the number of times each root appears in the dataframe\n",
        "- when several words are listed for the same root, I consider that the keyword associated with this root is the shortest name (this systematically selects the singular when there are singular/plural variants)\n",
        "\n",
        "The first step of the analysis is to retrieve the list of products:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f4da3052-c465-47bf-9652-a10a8ac51eb6",
        "_kg_hide-input": true,
        "_uuid": "1239a65ae122b1c020db626e5167451a950d8226",
        "id": "QpNk5AHdQyxZ"
      },
      "source": [
        "df_produits = pd.DataFrame(df_initial['Description'].unique()).rename(columns = {0:'Description'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAYcL-ovQyxd"
      },
      "source": [
        "Once this list is created, I use the function I previously defined in order to analyze the description of the various products:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCiz42yuSzc9"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f52a4134-c9c7-4d17-8510-8f55b1530cbb",
        "_kg_hide-input": true,
        "_uuid": "38c4872616b2c40bf69982070165cc9db3d0ea69",
        "id": "r5Q0PD3jQyxm"
      },
      "source": [
        "keywords, keywords_roots, keywords_select, count_keywords = keywords_inventory(df_produits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XufDE36Qyx4"
      },
      "source": [
        "The execution of this function returns three variables:\n",
        "- `keywords`: the list of extracted keywords\n",
        "- `keywords_roots`: a dictionary where the keys are the keywords roots and the values are the lists of words associated with those roots\n",
        "- `count_keywords`: dictionary listing the number of times every word is used\n",
        "\n",
        "At this point, I convert the `count_keywords` dictionary into a list, to sort the keywords according to their occurrence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "e033781a-8038-4302-93ed-78966554b7cc",
        "_kg_hide-input": true,
        "_uuid": "66fb955b137916f16d838f95a6f5bbe5e4952334",
        "id": "B1NkADBnQyx5"
      },
      "source": [
        "list_products = []\n",
        "for k,v in count_keywords.items():\n",
        "    list_products.append([keywords_select[k],v])\n",
        "list_products.sort(key = lambda x:x[1], reverse = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ri5QYtcQyx-"
      },
      "source": [
        "Using it, I create a representation of the most common keywords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "d6c78812-343e-41af-8e4e-ef0292dc4f7e",
        "_kg_hide-input": true,
        "_uuid": "fcdf4d98e372a1d65c931b7a6d5c29f269938022",
        "id": "sWzlTQIyQyyK"
      },
      "source": [
        "liste = sorted(list_products, key = lambda x:x[1], reverse = True)\n",
        "\n",
        "plt.rc('font', weight='normal')\n",
        "fig, ax = plt.subplots(figsize=(7, 25))\n",
        "y_axis = [i[1] for i in liste[:125]]\n",
        "x_axis = [k for k,i in enumerate(liste[:125])]\n",
        "x_label = [i[0] for i in liste[:125]]\n",
        "plt.xticks(fontsize = 15)\n",
        "plt.yticks(fontsize = 13)\n",
        "plt.yticks(x_axis, x_label)\n",
        "plt.xlabel(\"Number of occurences\", fontsize = 18, labelpad = 10)\n",
        "ax.barh(x_axis, y_axis, align = 'center')\n",
        "ax = plt.gca()\n",
        "ax.invert_yaxis()\n",
        "\n",
        "plt.title(\"Words occurence\",bbox={'facecolor':'k', 'pad':5}, color='w',fontsize = 25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwMhoxkyQyyW"
      },
      "source": [
        "### Defining product categories \n",
        "\n",
        "The list that was obtained contains more than 1400 keywords and the most frequent ones appear in more than 200 products. However, while examining the content of the list, I note that some names are useless. Others are do not carry information, like colors. Therefore, I discard these words from the analysis that follows and also, I decide to consider only the words that appear more than 13 times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "43300478-3b5a-4c7a-9466-34c384ccae60",
        "_kg_hide-input": true,
        "_uuid": "5f42482995f36f15688d8cba7f903e1277da5f92",
        "id": "GjSHKoGpQyyX"
      },
      "source": [
        "list_products = []\n",
        "for k,v in count_keywords.items():\n",
        "    word = keywords_select[k]\n",
        "    if word in ['pink', 'blue', 'tag', 'green', 'orange']: continue\n",
        "    if len(word) < 3 or v < 13: continue\n",
        "    if ('+' in word) or ('/' in word): continue\n",
        "    list_products.append([word, v])\n",
        " \n",
        "list_products.sort(key = lambda x:x[1], reverse = True)\n",
        "print('Preserved words:', len(list_products))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlqhiODHQyyy"
      },
      "source": [
        "#### Data encoding\n",
        "\n",
        "Now I will use these keywords to create groups of product. Firstly, I define the $X$ matrix as:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3gPElyWQyy1"
      },
      "source": [
        "   \n",
        "|   | word  1  |  ...  | word j  | ...  | word N  |\n",
        "|:-:|---|---|---|---|---|\n",
        "| product 1  | $a_{1,1}$  |     |   |   | $a_{1,N}$  |\n",
        "| ...        |            |     | ...  |   |   |\n",
        "|product i   |    ...     |     | $a_{i,j}$    |   | ...  |\n",
        "|...         |            |     |  ... |   |   |\n",
        "| product M  | $a_{M,1}$  |     |   |   | $a_{M,N}$   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvedV1EKQyy2"
      },
      "source": [
        "where the $a_ {i, j}$ coefficient  is 1 if the description of the product $i$ contains the word $j$, and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "23c77363-438b-4694-9b52-960c0ab3aa82",
        "_kg_hide-input": true,
        "_uuid": "d6faac7eb01d2251fb221569b75e007c8d5146aa",
        "id": "SgNF8JEQQyy3"
      },
      "source": [
        "liste_produits = df_cleaned['Description'].unique()\n",
        "#print(liste_produits[0:2])\n",
        "X = pd.DataFrame()\n",
        "for key, occurence in list_products:\n",
        "    X.loc[:, key] = list(map(lambda x:int(key.upper() in x), liste_produits))\n",
        "#print(X[0:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc-4Eqn6Qyy9"
      },
      "source": [
        "* The $X$ matrix indicates the words contained in the description of the products using the *one-hot-encoding* principle. \n",
        "\n",
        "\n",
        "\n",
        "* In practice, I have found that introducing the price range results in more balanced groups in terms of element numbers.\n",
        "Hence, I add 6 extra columns to this matrix, where I indicate the price range of the products:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "739c9cb1-3278-4d3a-a412-25dd2e8bc7c4",
        "_kg_hide-input": true,
        "_uuid": "b26a6bc55b61d0bcaccaf7c303b90d2a04636ac4",
        "id": "0euAxVwEQyy-"
      },
      "source": [
        "threshold = [0, 1, 2, 3, 5, 10]\n",
        "label_col = []\n",
        "for i in range(len(threshold)):\n",
        "    if i == len(threshold)-1:\n",
        "        col = '.>{}'.format(threshold[i])\n",
        "    else:\n",
        "        col = '{}<.<{}'.format(threshold[i],threshold[i+1])\n",
        "    #print(i)\n",
        "    #print(col)\n",
        "    label_col.append(col)\n",
        "    X.loc[:, col] = 0\n",
        "\n",
        "for i, prod in enumerate(liste_produits):\n",
        "    prix = df_cleaned[ df_cleaned['Description'] == prod]['UnitPrice'].mean()\n",
        "    #print (prix)\n",
        "    j = 0\n",
        "    while prix > threshold[j]:\n",
        "        j+=1\n",
        "        if j == len(threshold): break\n",
        "    X.loc[i, label_col[j-1]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6OkeF2FQyzw"
      },
      "source": [
        "and to choose the appropriate ranges, I check the number of products in the different groups:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "8587b9a3-d3a6-41ed-ac8a-9e0cdc9af31e",
        "_kg_hide-input": true,
        "_uuid": "6ad9fc6ed72057c63594d9d97d89742202f064f7",
        "id": "868A-fwdQyzx"
      },
      "source": [
        "print(\"{:<8} {:<20} \\n\".format('range', 'number of products') + 20*'-')\n",
        "for i in range(len(threshold)):\n",
        "    if i == len(threshold)-1:\n",
        "        col = '.>{}'.format(threshold[i])\n",
        "    else:\n",
        "        col = '{}<.<{}'.format(threshold[i],threshold[i+1])    \n",
        "    print(\"{:<10}  {:<20}\".format(col, X.loc[:, col].sum()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r12wo6ujQy0B"
      },
      "source": [
        "#### Creating clusters of products\n",
        "\n",
        "In this section, I will group the products into different classes. In the case of matrices with binary encoding, the most suitable metric for the calculation of distances is the [Hamming's metric](https://en.wikipedia.org/wiki/Distance_de_Hamming). Note that the **kmeans** method of sklearn uses a Euclidean distance that can be used, but it is not to the best choice in the case of categorical variables. However, in order to use the Hamming's metric, we need to use the [kmodes](https://pypi.python.org/pypi/kmodes/) package which is not available on the current plateform. Hence, I use the **kmeans** method even if this is not the best choice.\n",
        "\n",
        "In order to define (approximately) the number of clusters that best represents the data, I use the silhouette score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1c81a7c3-8980-4941-9082-7bf5cf92fc14",
        "_kg_hide-input": true,
        "_uuid": "4ce8586584935e81b9e403ea7a1dd4b2e4c9992e",
        "id": "1UGf8VIWQy0C"
      },
      "source": [
        "matrix = X.values\n",
        "for n_clusters in range(3,10):\n",
        "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n",
        "    kmeans.fit(matrix)\n",
        "    clusters = kmeans.predict(matrix)\n",
        "    silhouette_avg = silhouette_score(matrix, clusters)\n",
        "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjwzucH3Qy0M"
      },
      "source": [
        "In practice, the scores obtained above can be considered equivalent since, depending on the run, scores of $ 0.1 \\pm 0.05 $ will be obtained for all clusters with `n_clusters` $> $ 3 (we obtain slightly lower scores for the first cluster). On the other hand, I found that beyond 5 clusters, some clusters contained very few elements. I therefore choose to separate the dataset into 5 clusters. In order to ensure a good classification at every run of the notebook, I iterate untill we obtain the best possible silhouette score, which is, in the present case, around 0.15:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b29b3bb2-ef8d-4d7f-a19e-52045018ab8e",
        "_kg_hide-input": true,
        "_uuid": "72dd5bdab528264518bf034b84f66f3c29500fca",
        "id": "s6qUcECYQy0N"
      },
      "source": [
        "n_clusters = 5\n",
        "silhouette_avg = -1\n",
        "while silhouette_avg < 0.145:\n",
        "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n",
        "    kmeans.fit(matrix)\n",
        "    clusters = kmeans.predict(matrix)\n",
        "    silhouette_avg = silhouette_score(matrix, clusters)\n",
        "    \n",
        "    #km = kmodes.KModes(n_clusters = n_clusters, init='Huang', n_init=2, verbose=0)\n",
        "    #clusters = km.fit_predict(matrix)\n",
        "    #silhouette_avg = silhouette_score(matrix, clusters)\n",
        "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfcvMc5-Qy0T"
      },
      "source": [
        "#### Characterizing the content of clusters\n",
        "\n",
        "I check the number of elements in every class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "ad66161e-87ba-42ca-8128-bda16b470a34",
        "_kg_hide-input": true,
        "_uuid": "83591dd72975afde6a85d70429add02bd278c125",
        "id": "95ncxEekQy1I"
      },
      "source": [
        "pd.Series(clusters).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM5GxLkWQy1X"
      },
      "source": [
        "** a: _Silhouette intra-cluster score_ **\n",
        "\n",
        "In order to have an insight on the quality of the classification, we can represent the silhouette scores of each element of the different clusters. This is the purpose of the next figure which is taken from the [sklearn documentation](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c2b26710-2269-47e5-8f0d-98f6eac0015a",
        "_kg_hide-input": true,
        "_uuid": "e9e48bbbc3bb0ffa8134175aefb1fe808dea33e8",
        "id": "tJ3-ornIQy1Y"
      },
      "source": [
        "def graph_component_silhouette(n_clusters, lim_x, mat_size, sample_silhouette_values, clusters):\n",
        "    #plt.rcParams[\"patch.force_edgecolor\"] = True\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n",
        "    \n",
        "    fig, ax1 = plt.subplots(1, 1)\n",
        "    fig.set_size_inches(8, 8)\n",
        "    ax1.set_xlim([lim_x[0], lim_x[1]])\n",
        "    ax1.set_ylim([0, mat_size + (n_clusters + 1) * 10])\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        \n",
        "        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[clusters == i]\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "        #color = cm.spectral(float(i) / n_clusters) facecolor=color, edgecolor=color,       \n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, alpha=0.8)\n",
        "        \n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.03, y_lower + 0.5 * size_cluster_i, str(i), color = 'red', fontweight = 'bold',\n",
        "                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round, pad=0.3'))\n",
        "       \n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "8ee417cb-29d9-4913-8e12-0c020ecde059",
        "_kg_hide-input": true,
        "_uuid": "cf9b40ed0d865401e4a05e06eec2bff97ace7059",
        "id": "6Gy0sUSoQy1g"
      },
      "source": [
        "\n",
        "# define individual silouhette scores\n",
        "sample_silhouette_values = silhouette_samples(matrix, clusters)\n",
        "\n",
        "# and do the graph\n",
        "graph_component_silhouette(n_clusters, [-0.07, 0.33], len(X), sample_silhouette_values, clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3CpFBiYQy1m"
      },
      "source": [
        "** b: _Word Cloud_**\n",
        "\n",
        "Now we can have a look at the type of objects that each cluster represents. In order to obtain a global view of their contents, I determine which keywords are the most frequent in each of them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "159600b4-def2-4e4d-be54-6d84938721c4",
        "_kg_hide-input": true,
        "_uuid": "848009ae647f20366e148eeae96a4aa02f975618",
        "id": "xJonehc6Qy1n"
      },
      "source": [
        "liste = pd.DataFrame(liste_produits)\n",
        "liste_words = [word for (word, occurence) in list_products]\n",
        "\n",
        "occurence = [dict() for _ in range(n_clusters)]\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    liste_cluster = liste.loc[clusters == i]\n",
        "    for word in liste_words:\n",
        "        if word in ['art', 'set', 'heart', 'pink', 'blue', 'tag']: continue\n",
        "        occurence[i][word] = sum(liste_cluster.loc[:, 0].str.contains(word.upper()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw4G8RgCQy1r"
      },
      "source": [
        "and I output the result as wordclouds:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "2995e126-925b-436f-8b68-55d874637b1e",
        "_kg_hide-input": true,
        "_uuid": "3d88a32f7998249ce42e267bb912acc7420f0c47",
        "id": "PCNFDKezQy1v"
      },
      "source": [
        "\n",
        "def random_color_func(word=None, font_size=None, position=None,\n",
        "                      orientation=None, font_path=None, random_state=None):\n",
        "    h = int(360.0 * tone / 255.0)\n",
        "    s = int(100.0 * 255.0 / 255.0)\n",
        "    l = int(100.0 * float(random_state.randint(70, 120)) / 255.0)\n",
        "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
        "\n",
        "def make_wordcloud(liste, increment):\n",
        "    ax1 = fig.add_subplot(4,2,increment)\n",
        "    words = dict()\n",
        "    trunc_occurences = liste[0:150]\n",
        "    for s in trunc_occurences:\n",
        "        words[s[0]] = s[1]\n",
        "    \n",
        "    wordcloud = WordCloud(width=1000,height=400, background_color='lightgrey', \n",
        "                          max_words=1628,relative_scaling=1,\n",
        "                          color_func = random_color_func,\n",
        "                          normalize_plurals=False)\n",
        "    wordcloud.generate_from_frequencies(words)\n",
        "    ax1.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    ax1.axis('off')\n",
        "    plt.title('cluster n{}'.format(increment-1))\n",
        "\n",
        "fig = plt.figure(1, figsize=(14,14))\n",
        "color = [0, 160, 130, 95, 280, 40, 330, 110, 25]\n",
        "for i in range(n_clusters):\n",
        "    list_cluster_occurences = occurence[i]\n",
        "\n",
        "    tone = color[i] # define the color of the words\n",
        "    liste = []\n",
        "    for key, value in list_cluster_occurences.items():\n",
        "        liste.append([key, value])\n",
        "    liste.sort(key = lambda x:x[1], reverse = True)\n",
        "    make_wordcloud(liste, i+1)            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQdBSWrzQy13"
      },
      "source": [
        "From this representation, we can see that for example, one of the clusters contains objects that could be associated with gifts (keywords: Christmas, packaging, card, ...). Another cluster would rather contain luxury items and jewelry (keywords: necklace, bracelet, lace, silver, ...). Nevertheless, it can also be observed that many words appear in various clusters and it is therefore difficult to clearly distinguish them.\n",
        "\n",
        "** c: _Principal Component Analysis_ **\n",
        "\n",
        "In order to ensure that these clusters are truly distinct, I look at their composition. Given the large number of variables of the initial matrix, I first perform a PCA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "85af75c5-bf90-4b8c-9687-caff723d0027",
        "_kg_hide-input": true,
        "_uuid": "33859d205bcf40477d166b679fb97996ba2d5f48",
        "id": "sPCCPsSMQy15"
      },
      "source": [
        "pca = PCA()\n",
        "pca.fit(matrix)\n",
        "pca_samples = pca.transform(matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33VLkcMvQy2F"
      },
      "source": [
        "and then check for the amount of variance explained by each component:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "41cd8738-4923-43cd-b26a-fba6d53070e8",
        "_kg_hide-input": true,
        "_uuid": "abce52d76e801aa6197603925618fb032aec78af",
        "id": "5pw7MylcQy2G"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(14, 5))\n",
        "sns.set(font_scale=1)\n",
        "plt.step(range(matrix.shape[1]), pca.explained_variance_ratio_.cumsum(), where='mid',\n",
        "         label='cumulative explained variance')\n",
        "sns.barplot(np.arange(1,matrix.shape[1]+1), pca.explained_variance_ratio_, alpha=0.5, color = 'g',\n",
        "            label='individual explained variance')\n",
        "plt.xlim(0, 100)\n",
        "\n",
        "ax.set_xticklabels([s if int(s.get_text())%2 == 0 else '' for s in ax.get_xticklabels()])\n",
        "\n",
        "plt.ylabel('Explained variance', fontsize = 14)\n",
        "plt.xlabel('Principal components', fontsize = 14)\n",
        "plt.legend(loc='upper left', fontsize = 13);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apoWbsbvQy2M"
      },
      "source": [
        "We see that the number of components required to explain the data is extremely important: we need more than 100 components to explain 90% of the variance of the data. In practice, I decide to keep only a limited number of components since this decomposition is only performed to visualize the data:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "0e49b019-d1b7-4815-8c5e-7cf27fa5b5b2",
        "_kg_hide-input": true,
        "_uuid": "fb0afe8523c2634860fdaf67d734c1dc0897c4c0",
        "id": "23zF1VQDQy2P"
      },
      "source": [
        "pca = PCA(n_components=50)\n",
        "matrix_9D = pca.fit_transform(matrix)\n",
        "mat = pd.DataFrame(matrix_9D)\n",
        "mat['cluster'] = pd.Series(clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "675cc670-4512-4983-8b58-e82f3cf2bf3d",
        "_kg_hide-input": true,
        "_uuid": "b9d872fa5038458f3424dfc585a5a823efc7ff7f",
        "id": "S6JEswGeQy2X"
      },
      "source": [
        "import matplotlib.patches as mpatches\n",
        "\n",
        "sns.set_style(\"white\")\n",
        "sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})\n",
        "\n",
        "LABEL_COLOR_MAP = {0:'r', 1:'gold', 2:'b', 3:'k', 4:'c', 5:'g'}\n",
        "label_color = [LABEL_COLOR_MAP[l] for l in mat['cluster']]\n",
        "\n",
        "fig = plt.figure(figsize = (12,10))\n",
        "increment = 0\n",
        "for ix in range(4):\n",
        "    for iy in range(ix+1, 4):    \n",
        "        increment += 1\n",
        "        ax = fig.add_subplot(3,3,increment)\n",
        "        ax.scatter(mat[ix], mat[iy], c= label_color, alpha=0.4) \n",
        "        plt.ylabel('PCA {}'.format(iy+1), fontsize = 12)\n",
        "        plt.xlabel('PCA {}'.format(ix+1), fontsize = 12)\n",
        "        ax.yaxis.grid(color='lightgray', linestyle=':')\n",
        "        ax.xaxis.grid(color='lightgray', linestyle=':')\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        \n",
        "        if increment == 9: break\n",
        "    if increment == 9: break\n",
        "        \n",
        "\n",
        "comp_handler = []\n",
        "for i in range(5):\n",
        "    comp_handler.append(mpatches.Patch(color = LABEL_COLOR_MAP[i], label = i))\n",
        "\n",
        "plt.legend(handles=comp_handler, bbox_to_anchor=(1.1, 0.97), \n",
        "           title='Cluster',\n",
        "           shadow = True, frameon = True, framealpha = 1,fontsize = 13, \n",
        "           bbox_transform = plt.gcf().transFigure) #facecolor = 'lightgrey',\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}